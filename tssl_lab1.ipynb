{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSSL Lab 1 - Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a few packages that are useful for solvign this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  # Loading data / handling data frames\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model as lm  # Used for solving linear regression problems\n",
    "from sklearn.neural_network import MLPRegressor # Used for NAR model\n",
    "\n",
    "from tssltools_lab1 import acf, acfplot # Module available in LISAM - Used for plotting ACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading, plotting and detrending data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will build autoregressive models for a data set corresponding to the Global Mean Sea Level (GMSL) over the past few decades. The data is taken from https://climate.nasa.gov/vital-signs/sea-level/ and is available on LISAM in the file `sealevel.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Load the data and plot the GMSL versus time. How many observations are there in total in this data set?\n",
    "\n",
    "_Hint:_ With pandas you can use the function `pandas.read_csv` to read the csv file into a data frame. Plotting the time series can be done using `pyplot`. Note that the sea level data is stored in the 'GMSL' column and the time when each data point was recorded is stored in the column 'Year'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: The data has a clear upward trend. Before fitting an AR model to this data need to remove this trend. Explain, using one or two sentences, why this is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** Detrend the data following these steps:\n",
    "1. Fit a straight line, $\\mu_t=\\theta_0 + \\theta_1 u_t $ to the data based on the method of least squares. Here, $u_t$ is the time point when obervation $t$ was recorded.\n",
    "\n",
    "    _Hint:_ You can use `lm.LinearRegression().fit(...)` from scikit-learn. Note that the inputs need to be passed as a 2D array.\n",
    "\n",
    "    Before going on to the next step, plot your fitted line and the data in one figure.\n",
    "\n",
    "\n",
    "2. Subtract the fitted line from $y_t$ for the whole data series and plot the deviations from the straight line.\n",
    "\n",
    "**From now, we will use the detrended data in all parts of the lab.**\n",
    "\n",
    "_Note:_ The GMSL data is recorded at regular time intervals, so that $u_{t+1} - u_t = $ const. Therefore, you can just as well use $t$ directly in the linear regression function if you prefer, $\\mu_t=\\theta_0 + \\theta_1 t $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** Split the (detrended) time series into training and validation sets. Use the values from the beginning up to the 700th time point (i.e. $y_t$ for $t=1$ to $t=700$) as your training data, and the rest of the values as your validation data. Plot the two data sets.\n",
    "\n",
    "_Note:_ In the above, we have allowed ourselves to use all the available data (train + validation) when detrending. An alternative would be to use only the training data also when detrending the model. The latter approach is more suitable if, either:\n",
    "* we view the linear detrending as part of the model choice. Perhaps we wish to compare different polynomial trend models, and evaluate their performance on the validation data, or\n",
    "* we wish to use the second chunk of observations to estimate the performance of the final model on unseen data (in that case it is often referred to as \"test data\" instead of \"validation data\"), in which case we should not use these observations when fitting the model, including the detrending step.\n",
    "\n",
    "In this laboration we consider the linear detrending as a predetermined preprocessing step and therefore allow ourselves to use the validation data when computing the linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fit an autoregressive model\n",
    "We will now fit an AR$(p)$ model to the training data for a given value of the model order $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Create a function that fits an AR$(p)$ model for an arbitrary value of p. Use this function to fit a model of order $p=10$ to the training data and write out (or plot) the coefficients.\n",
    "\n",
    "_Hint:_ Since fitting an AR model is essentially just a standard linear regression we can make use of `lm.LinearRegression().fit(...)` similarly to above. You may use the template below and simply fill in the missing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ar(y, p):\n",
    "    \"\"\"Fits an AR(p) model. The loss function is the sum of squared errors from t=p+1 to t=n.\n",
    "\n",
    "    :param y: array (n,), training data points\n",
    "    :param p: int, AR model order\n",
    "    :return theta: array (p,), learnt AR coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training data points\n",
    "    n = ... # <COMPLETE THIS LINE>\n",
    "    \n",
    "    # Construct the regression matrix\n",
    "    Phi = np.zeros( ... # <COMPLETE THIS LINE>\n",
    "    for j in range(p):\n",
    "        Phi[:,j] = ... # <COMPLETE THIS LINE>\n",
    "    \n",
    "    # Drop the first p values from the target vector y\n",
    "    yy = y[p:]  # yy = (y_{t+p+1}, ..., y_n)\n",
    "\n",
    "    # Here we use fit_intercept=False since we do not want to include an intercept term in the AR model\n",
    "    regr = lm.LinearRegression(fit_intercept=False)\n",
    "    regr.fit(Phi,yy)    \n",
    "\n",
    "    return regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Next, write a function that computes the one-step-ahead prediction of your fitted model. 'One-step-ahead' here means that in order to predict $y_t$ at $t=t_0$, we use the actual values of $y_t$ for $t<t_0$ from the data. Use your function to compute the predictions for both *training data* and *validation data*. Plot the predictions together with the data (you can plot both training and validation data in the same figure). Also plot the *residuals*.\n",
    "\n",
    "_Hint:_ It is enought to call the predict function once, for both training and validation data at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A6:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ar_1step(theta, y_target):\n",
    "    \"\"\"Predicts the value y_t for t = p+1, ..., n, for an AR(p) model, based on the data in y_target using\n",
    "    one-step-ahead prediction.\n",
    "\n",
    "    :param theta: array (p,), AR coefficients, theta=(a1,a2,...,ap).\n",
    "    :param y_target: array (n,), the data points used to compute the predictions.\n",
    "    :return y_pred: array (n-p,), the one-step predictions (\\hat y_{p+1}, ...., \\hat y_n) \n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y_target)\n",
    "    p = len(theta)\n",
    "    \n",
    "    # Number of steps in prediction\n",
    "    m = n-p\n",
    "    y_pred = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        # <COMPLETE THIS CODE BLOCK>\n",
    "        y_pred[i] = ... # <COMPLETE THIS LINE>\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Compute and plot the autocorrelation function (ACF) of the *residuals* only for the *validation data*. What conclusions can you draw from the ACF plot?\n",
    "\n",
    "_Hint:_ You can use the function `acfplot` from the `tssltools` module, available on the course web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function acfplot in module tssltools_lab1:\n",
      "\n",
      "acfplot(x, lags=None, conf=0.95)\n",
      "    Plots the empirical autocorralation function.\n",
      "    \n",
      "    :param x: array (n,), sequence of data points\n",
      "    :param lags: int, maximum lag to compute the ACF for. If None, this is set to n. Default is None.\n",
      "    :param conf: float, number in the interval [0,1] which specifies the confidence level (based on a central limit\n",
      "                 theorem under a white noise assumption) for two dashed lines drawn in the plot. Default is 0.95.\n",
      "    :return:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(acfplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model validation and order selection\n",
    "Above we set the model order $p=10$ quite arbitrarily. In this section we will try to find an appropriate order by validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**: Write a loop in which AR-models of orders from $p=2$ to $p=150$ are fitted to the data above. Plot the training and validation mean-squared errors for the one-step-ahead predictions versus the model order.\n",
    "\n",
    "Based on your results:\n",
    "- What is the main difference between the changes in training error and validation error as the order increases? \n",
    "- Based on these results, which model order would you suggest to use and why?\n",
    "\n",
    "_Note:_ There is no obvious \"correct answer\" to the second question, but you still need to pick an order an motivate your choice!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A8:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Based on the chosen model order, compute the residuals of the one-step-ahead predictions on the *validation data*. Plot the autocorrelation function of the residuals. What conclusions can you draw? Compare to the ACF plot generated above for p=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Long-range predictions\n",
    "So far we have only considered one-step-ahead predictions. However, in many practical applications it is of interest to use the model to predict further into the future. For intance, for the sea level data studied in this laboration, it is more interesting to predict the level one year from now, and not just 10 days ahead (10 days = 1 time step in this data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**: \n",
    "Write a function that simulates the value of an AR($p$) model $m$ steps into the future, conditionally on an initial sequence of data points. Specifically, given $y_{1:n}$ with $n\\geq p$ the function/code should predict the values\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat y_{t|n} &= \\mathbb{E}[y_{t} | y_{1:n}], & t&=n+1,\\dots,n+m.\n",
    "\\end{align}\n",
    "\n",
    "Use this to predict the values for the validation data ($y_{701:997}$) conditionally on the training data ($y_{1:700}$) and plot the result.\n",
    "\n",
    "_Hint:_ Use the pseudo-code derived at the first pen-and-paper session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A10:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar(y, theta, m):\n",
    "    \"\"\"Simulates an AR(p) model for m steps, with initial condition given by the last p values of y\n",
    "    \n",
    "    :param y: array (n,) with n>=p. The last p values are used to initialize the simulation.\n",
    "    :param theta: array (p,). AR model parameters,\n",
    "    :param m: int, number of time steps to simulate the model for.\n",
    "    \"\"\"\n",
    "\n",
    "    p = len(theta)    \n",
    "    y_sim = np.zeros(m)\n",
    "    phi = np.flip(y[-p:].copy()) # (y_{n-1}, ..., y_{n-p})^T - note that y[ntrain-1] is the last training data point\n",
    "\n",
    "    for i in range(m):\n",
    "        y_sim[i] = ... # <COMPLETE THIS LINE>\n",
    "        # <COMPLETE THIS CODE BLOCK>\n",
    "    \n",
    "    return y_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11:** Using the same function as above, try to simulate the process for a large number of time steps (say, $m=2000$). You should see that the predicted values eventually converge to a constant prediction of zero. Is this something that you would expect to see in general? Explain the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A11:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Nonlinear AR model\n",
    " In this part, we switch to a nonlinear autoregressive (NAR) model, which is based on a feedforward neural network. This means that in this model the recursive equation for making predictions is still in the form $\\hat y_t=f_\\theta(y_{t-1},...,y_{t-p})$, but this time $f$ is a nonlinear function learned by the neural network. Fortunately almost all of the work for implementing the neural network and training it is handled by the `scikit-learn` package with a few lines of code, and we just need to choose the right structure, and prepare the input-output data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12**: Construct a NAR($p$) model with a feedforward (MLP) network, by using the `MLPRegressor` class from `scikit-learn`. Set $p$ to the same value as you chose for the linear AR model above. Initially, you can use an MLP with a single hidden layer consisting of 10 hidden neurons. \n",
    "Train it using the same training data as above and plot the one-step-ahead predictions as well as the residuals, on both the training and validation data. \n",
    "\n",
    "_Hint:_ You will need the methods `fit` and `predict` of `MLPRegressor`. Read the user guide of `scikit-learn` for more details. Recall that a NAR model is conceptuall very similar to an AR model, so you can reuse part of the code from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A12:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13:** Try to expirement with different choices for the hyperparameters of the network (e.g. number of hidden layers and units per layer, activation function, etc.) and the optimizer (e.g. `solver` and `max_iter`).\n",
    "\n",
    "Are you satisfied with the results? Why/why not? Discuss what the limitations of this approach might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A13:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
